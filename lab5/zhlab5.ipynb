{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dff239a",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaaf75c",
   "metadata": {},
   "source": [
    "# 1.–¶–µ–ª—å –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c87ace",
   "metadata": {},
   "source": [
    "–∏–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab617be",
   "metadata": {},
   "source": [
    "# 2.–ó–∞–¥–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dfcb8",
   "metadata": {},
   "source": [
    "1.–î–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ —Ä–µ—à–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600c1b3",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è.\n",
    "\n",
    "–ß–∞—Å—Ç–µ—Ä–µ—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.\n",
    "\n",
    "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è.\n",
    "\n",
    "–í—ã–¥–µ–ª–µ–Ω–∏–µ (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ) –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–†–∞–∑–±–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d528fd",
   "metadata": {},
   "source": [
    "2.–î–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0efbb",
   "metadata": {},
   "source": [
    "–°–ø–æ—Å–æ–± 1. –ù–∞ –æ—Å–Ω–æ–≤–µ CountVectorizer –∏–ª–∏ TfidfVectorizer.\n",
    "\n",
    "–°–ø–æ—Å–æ–± 2. –ù–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π word2vec –∏–ª–∏ Glove –∏–ª–∏ fastText.\n",
    "\n",
    "–°—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644a0e3",
   "metadata": {},
   "source": [
    "–î–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ \"datasets for text classification\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4950203",
   "metadata": {},
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582559a",
   "metadata": {},
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90189332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting razdel\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: razdel\n",
      "Successfully installed razdel-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\software\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf0070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize, sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91027a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '–æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ (–Ω–æ–º–µ—Ä –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç—ã, –∏–º—è, –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏ –¥—Ä—É–≥–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è), –ø–æ–∏—Å–∫–∞, –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –∞ —Ç–∞–∫–∂–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce283e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 10, '–æ–±–µ—Å–ø–µ—á–∏—Ç—å'),\n",
       " Substring(11, 22, '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å'),\n",
       " Substring(23, 29, '–∑–∞–ø–∏—Å–∏'),\n",
       " Substring(30, 40, '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'),\n",
       " Substring(41, 42, '–æ'),\n",
       " Substring(43, 50, '–∫–ª–∏–µ–Ω—Ç–µ'),\n",
       " Substring(51, 52, '('),\n",
       " Substring(52, 57, '–Ω–æ–º–µ—Ä'),\n",
       " Substring(58, 68, '–±–∞–Ω–∫–æ–≤—Å–∫–æ–π'),\n",
       " Substring(69, 74, '–∫–∞—Ä—Ç—ã'),\n",
       " Substring(74, 75, ','),\n",
       " Substring(76, 79, '–∏–º—è'),\n",
       " Substring(79, 80, ','),\n",
       " Substring(81, 86, '–Ω–æ–º–µ—Ä'),\n",
       " Substring(87, 95, '—Ç–µ–ª–µ—Ñ–æ–Ω–∞'),\n",
       " Substring(96, 97, '–∏'),\n",
       " Substring(98, 104, '–¥—Ä—É–≥–∞—è'),\n",
       " Substring(105, 113, '–æ—Å–Ω–æ–≤–Ω–∞—è'),\n",
       " Substring(114, 124, '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'),\n",
       " Substring(124, 125, ')'),\n",
       " Substring(125, 126, ','),\n",
       " Substring(127, 133, '–ø–æ–∏—Å–∫–∞'),\n",
       " Substring(133, 134, ','),\n",
       " Substring(135, 143, '–∫–æ–Ω—Ç—Ä–æ–ª—è'),\n",
       " Substring(144, 151, '–¥–æ—Å—Ç—É–ø–∞'),\n",
       " Substring(152, 153, '–∏'),\n",
       " Substring(154, 165, '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞'),\n",
       " Substring(165, 166, ','),\n",
       " Substring(167, 168, '–∞'),\n",
       " Substring(169, 174, '—Ç–∞–∫–∂–µ'),\n",
       " Substring(175, 185, '—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è'),\n",
       " Substring(186, 194, '–∂—É—Ä–Ω–∞–ª–æ–º'),\n",
       " Substring(195, 206, '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tok_text = list(tokenize(text))\n",
    "n_tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79d2a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–æ–±–µ—Å–ø–µ—á–∏—Ç—å',\n",
       " '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å',\n",
       " '–∑–∞–ø–∏—Å–∏',\n",
       " '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',\n",
       " '–æ',\n",
       " '–∫–ª–∏–µ–Ω—Ç–µ',\n",
       " '(',\n",
       " '–Ω–æ–º–µ—Ä',\n",
       " '–±–∞–Ω–∫–æ–≤—Å–∫–æ–π',\n",
       " '–∫–∞—Ä—Ç—ã',\n",
       " ',',\n",
       " '–∏–º—è',\n",
       " ',',\n",
       " '–Ω–æ–º–µ—Ä',\n",
       " '—Ç–µ–ª–µ—Ñ–æ–Ω–∞',\n",
       " '–∏',\n",
       " '–¥—Ä—É–≥–∞—è',\n",
       " '–æ—Å–Ω–æ–≤–Ω–∞—è',\n",
       " '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',\n",
       " ')',\n",
       " ',',\n",
       " '–ø–æ–∏—Å–∫–∞',\n",
       " ',',\n",
       " '–∫–æ–Ω—Ç—Ä–æ–ª—è',\n",
       " '–¥–æ—Å—Ç—É–ø–∞',\n",
       " '–∏',\n",
       " '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞',\n",
       " ',',\n",
       " '–∞',\n",
       " '—Ç–∞–∫–∂–µ',\n",
       " '—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è',\n",
       " '–∂—É—Ä–Ω–∞–ª–æ–º',\n",
       " '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in n_tok_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1860d8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           206,\n",
       "           '–æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ (–Ω–æ–º–µ—Ä –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç—ã, –∏–º—è, –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏ –¥—Ä—É–≥–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è), –ø–æ–∏—Å–∫–∞, –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –∞ —Ç–∞–∫–∂–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_text = list(sentenize(text))\n",
    "n_sen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af630bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['–æ–±–µ—Å–ø–µ—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ (–Ω–æ–º–µ—Ä –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç—ã, –∏–º—è, –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏ –¥—Ä—É–≥–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è), –ø–æ–∏—Å–∫–∞, –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –∞ —Ç–∞–∫–∂–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏'],\n",
       " 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in n_sen_text], len([_.text for _ in n_sen_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0248aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≠—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "def n_sentenize(text):\n",
    "    n_sen_chunk = []\n",
    "    for sent in sentenize(text):\n",
    "        tokens = [_.text for _ in tokenize(sent.text)]\n",
    "        n_sen_chunk.append(tokens)\n",
    "    return n_sen_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739775e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['–æ–±–µ—Å–ø–µ—á–∏—Ç—å',\n",
       "  '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å',\n",
       "  '–∑–∞–ø–∏—Å–∏',\n",
       "  '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',\n",
       "  '–æ',\n",
       "  '–∫–ª–∏–µ–Ω—Ç–µ',\n",
       "  '(',\n",
       "  '–Ω–æ–º–µ—Ä',\n",
       "  '–±–∞–Ω–∫–æ–≤—Å–∫–æ–π',\n",
       "  '–∫–∞—Ä—Ç—ã',\n",
       "  ',',\n",
       "  '–∏–º—è',\n",
       "  ',',\n",
       "  '–Ω–æ–º–µ—Ä',\n",
       "  '—Ç–µ–ª–µ—Ñ–æ–Ω–∞',\n",
       "  '–∏',\n",
       "  '–¥—Ä—É–≥–∞—è',\n",
       "  '–æ—Å–Ω–æ–≤–Ω–∞—è',\n",
       "  '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',\n",
       "  ')',\n",
       "  ',',\n",
       "  '–ø–æ–∏—Å–∫–∞',\n",
       "  ',',\n",
       "  '–∫–æ–Ω—Ç—Ä–æ–ª—è',\n",
       "  '–¥–æ—Å—Ç—É–ø–∞',\n",
       "  '–∏',\n",
       "  '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞',\n",
       "  ',',\n",
       "  '–∞',\n",
       "  '—Ç–∞–∫–∂–µ',\n",
       "  '—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è',\n",
       "  '–∂—É—Ä–Ω–∞–ª–æ–º',\n",
       "  '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_chunk = n_sentenize(text)\n",
    "n_sen_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9302f1",
   "metadata": {},
   "source": [
    "# 3.1 –î–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, —Ä–µ—à–∏—Ç–µ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380144e",
   "metadata": {},
   "source": [
    "–°–ø–æ—Å–æ–± 1. –ù–∞ –æ—Å–Ω–æ–≤–µ CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73f30bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\31139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "660dacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    y_true - –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "    y_pred - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: –∫–ª—é—á - –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞, \n",
    "    –∑–Ω–∞—á–µ–Ω–∏–µ - Accuracy –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    \"\"\"\n",
    "    # –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "    classes = np.unique(y_true)\n",
    "    # –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ª–æ–≤–∞—Ä—å\n",
    "    res = dict()\n",
    "    # –ü–µ—Ä–µ–±–æ—Ä –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "    for c in classes:\n",
    "        # –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç \n",
    "        # —Ç–µ–∫—É—â–µ–π –º–µ—Ç–∫–µ –∫–ª–∞—Å—Å–∞ –≤ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # —Ä–∞—Å—á–µ—Ç accuracy –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫–∏ accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('–ú–µ—Ç–∫–∞ \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f85bb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JLZlCZ0</td>\n",
       "      <td>Ep 1| Travelling through North East India | Of...</td>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9E_Blai8vk</td>\n",
       "      <td>Welcome to Bali | Travel Vlog | Priscilla Lee</td>\n",
       "      <td>Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r284c-q8oY</td>\n",
       "      <td>My Solo Trip to ALASKA | Cruising From Vancouv...</td>\n",
       "      <td>Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qmi-Xwq-ME</td>\n",
       "      <td>Traveling to the Happiest Country in the World!!</td>\n",
       "      <td>Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_lcOX55Ef70</td>\n",
       "      <td>Solo in Paro Bhutan | Tiger's Nest visit | Bhu...</td>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          link                                              title  \\\n",
       "0      JLZlCZ0  Ep 1| Travelling through North East India | Of...   \n",
       "1  i9E_Blai8vk      Welcome to Bali | Travel Vlog | Priscilla Lee   \n",
       "2   r284c-q8oY  My Solo Trip to ALASKA | Cruising From Vancouv...   \n",
       "3   Qmi-Xwq-ME   Traveling to the Happiest Country in the World!!   \n",
       "4  _lcOX55Ef70  Solo in Paro Bhutan | Tiger's Nest visit | Bhu...   \n",
       "\n",
       "                                         description category  \n",
       "0  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...   travel  \n",
       "1  Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...   travel  \n",
       "2  Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...   travel  \n",
       "3  Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...   travel  \n",
       "4  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...   travel  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"youtube.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7366940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   value\n",
       "0  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...  travel\n",
       "1  Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...  travel\n",
       "2  Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...  travel\n",
       "3  Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...  travel\n",
       "4  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...  travel"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#–¢–æ–ª—å–∫–æ –¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ \"verified_reviews\" –∏ \"feedback\".\n",
    "df_new = pd.DataFrame(df,columns=['description','category'])\n",
    "df_new.columns = ['text','value']\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a651639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e03d54bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*DISCLAIMER* Please do not ride elephants when visiting any country. At the time I didn't know (yes, I was dumb) so it is shown in the video, but I do not support the elephant riding business anymore. If I could take it back I would, but instead I want to pass on the knowledge to anyone who isn't aware. Here's some info: \\nSHOW MORE\",\n",
       " 'Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\nI spent 11 days cruising up the coast of Alaska and it was MAGICAL.\\n‚Ä¢ALASKA BLOG POST: https://allisonanderson.com/blog/crui...\\n‚Ä¢Adventures on INSTAGRAM http://www.instagram.com/photoallison\\nSHOW MORE',\n",
       " \"Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLACK FRIDAY DROP Out Now*: http://seek-discomfort.com/yes-theory \\nThis week only, with every purchase about $35, you'll get 2 free Seek Discomfort flags!\\n\\nCheck out our friends from Beautiful Destinations!! Their videos are INCREDIBLE:\\nSHOW MORE\",\n",
       " 'Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nHere‚Äôs presenting the first part of the Bhutan Series Episode in Paro. I went straight to Paro as first part of my road trip in the country. The drive from Phuntsholing took about 4 hours. \\n \\nThe entire budget of my Bhutan trip was close to  INR 25k. You can carry cash everywhere in Indian currency in Bhutan as it is accepted. Some things about Paro below:\\n\\n1. The place where I stayed at in Paro is called Ama‚Äôs Village Lodge. You can book the place here - \\nSHOW MORE',\n",
       " 'MOUNTAIN TREKKER\\n1.42M subscribers\\nJOIN\\nSUBSCRIBE\\nTHAILAND: FREE VISA ON ARRIVAL  \\nCheck this- https://www.youtube.com/channel/UCl5d...\\nSHOW MORE',\n",
       " \"XTREME MOTO ADVENTURE\\n800K subscribers\\nJOIN\\nSUBSCRIBE\\nAmazing Kerala story | Rainforest Athirapally | \\nHere's is episode 37 of my All India ride and you will be amazed by this episode. Kerala is really a god's own country..Check it out yourself.\\nGet 15 % Discount on Booking\\nCoupon Code : EXPLORE \\nRainforest link : \\nSHOW MORE\",\n",
       " 'visa2explore\\n1.21M subscribers\\nJOIN\\nSUBSCRIBE\\nHow to plan your journey of Meghalaya, North east India Tour. This video has information on Things to do in Meghalaya.\\n\\nNorth East India is known for its beauty, hills, waterfalls and so much more. After watching this video you will know about North east India tourism ie tourist places.\\n\\nThis video can help you plan out your North East India tour, starting from Meghalaya, then you can watch our series of Assam and Sikkim too.\\n\\nThis video on Meghalaya tourism has loads of knowledge on must visit Tourist destinations of Meghalaya. we spent 15 days in Meghalaya.\\n\\nWe traveled to all the 3 tribe areas of Meghalaya - khasi hills, Jaintia hills and Garo hills. Basis my experience i have shared with you a plan for 4 nights and 5 days in Meghalaya.\\n\\nImportant information below:\\n\\nPlaces to visit in Sohra ( also called Cherrapunji), basis time availability at your end your may prioritise, which places to visit and which ones to ignore.\\n\\nWah Kaba Falls\\nDainthlen Falls\\nWei Sawdong Falls\\nNohkalikai Falls\\nThe Seven Sisters Falls\\nKynrem Falls\\nMawsmai Cave\\nArwah Cave\\nDouble decker Living Root Bridge\\nRainbow Falls\\n\\nPlaces to visit in Dawki\\n\\nUmngot River (You can enjoy boating here)\\nShnongpdeng (You can enjoy water sports at this place)\\n\\nLamin Guest House: we stayed here, there are not many hotels in Dawki\\nRoom Tariff Rs 3000 for double occupancy (Room also starts from Rs 1500)\\nWebsite: \\nSHOW MORE',\n",
       " \"Garima's Good Life\\n1.76M subscribers\\nSUBSCRIBE\\nDo watch my video on how to prepare and pack for LADAKH trip if you plan to visit \\nhttps://youtu.be/TRUTQ7fd_XA\\nSHOW MORE\",\n",
       " \"Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nKerala is one of the most beautiful states in India for traveling and a road trip. In this series  we are going to cover 5 main destinations by road - Wayanad, Athirapally falls, Vagamon, Varkala, and Alleppey. In this episode, I'm in Varkala, with some of the best beaches! It has places like black sand beach, and North Cliff, restaurants like Inda Cafe and Kerala God's Own Country Kitchen. \\n\\nI actually traveled to Kerala from Karnataka and took an RT-PCR negative report with me while travelling and in this episode, I stayed at Cliff Stories, which is a really nice place for relaxing and staying at. üòÄ\\n\\nThis video is sponsored by Skillshare!\\nGive a new direction to your creative skills, first 5000 subscribers can avail a free trial on Skillshare using this link - \\nSHOW MORE\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#–°—Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å\n",
    "vocab_list = df_new['text'].tolist()\n",
    "vocab_list[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72d8a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - 22038\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),  #ngram_range=(1, 1) is the default\n",
    "    dtype='double'\n",
    ")\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e9d88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khanijow=10716\n",
      "671k=1257\n",
      "subscribers=18036\n",
      "subscribe=18030\n",
      "journey=10329\n",
      "arunachal=2552\n",
      "north=13431\n",
      "east=6607\n",
      "india=9566\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6784b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = vocabVect.transform(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db93f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3599x22038 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114891 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e310156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83e7ff9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22038"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –†–∞–∑–º–µ—Ä –Ω—É–ª–µ–≤–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6090c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ù–µ–ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª–µ–≤–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "[i for i in test_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85fd857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['11204',\n",
       " '11261',\n",
       " '112m',\n",
       " '113',\n",
       " '113k',\n",
       " '114',\n",
       " '114k',\n",
       " '115',\n",
       " '115k',\n",
       " '116',\n",
       " '1160',\n",
       " '116k',\n",
       " '117',\n",
       " '117k',\n",
       " '118',\n",
       " '1185',\n",
       " '119',\n",
       " '11k',\n",
       " '11m',\n",
       " '11th']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dcd8b",
   "metadata": {},
   "source": [
    "–†–∞–∑–¥–µ–ª–∏–º –≤—ã–±–æ—Ä–∫—É –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e10e0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_new['text'], df_new['value'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbeaf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22136f",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä \"LogisticRegression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26662231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç–∫–∞ \t Accuracy\n",
      "art_music \t 0.9253731343283582\n",
      "food \t 0.8598484848484849\n",
      "history \t 0.8531073446327684\n",
      "travel \t 0.9191374663072777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LogisticRegression(C=3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00bab4",
   "metadata": {},
   "source": [
    "–°–ø–æ—Å–æ–± 2. –ù–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b76070e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edd4b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –∫–æ—Ä–ø—É—Å\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in df_new['text'].values:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb573f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tanya',\n",
       "  'khanijow',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'journey',\n",
       "  'arunachal',\n",
       "  'north',\n",
       "  'east',\n",
       "  'india',\n",
       "  'begins',\n",
       "  'train',\n",
       "  'journey',\n",
       "  'guwahati',\n",
       "  'murkongselek',\n",
       "  'head',\n",
       "  'pasighat',\n",
       "  'travel',\n",
       "  'companions',\n",
       "  'getting',\n",
       "  'started',\n",
       "  'exploring',\n",
       "  'tiny',\n",
       "  'glimpse',\n",
       "  'arunachal',\n",
       "  'far',\n",
       "  'markets',\n",
       "  'bridges',\n",
       "  'adventure',\n",
       "  'get',\n",
       "  'better',\n",
       "  'next',\n",
       "  'video',\n",
       "  'show'],\n",
       " ['priscilla',\n",
       "  'lee',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'disclaimer',\n",
       "  'please',\n",
       "  'ride',\n",
       "  'elephants',\n",
       "  'visiting',\n",
       "  'country',\n",
       "  'time',\n",
       "  'know',\n",
       "  'yes',\n",
       "  'dumb',\n",
       "  'shown',\n",
       "  'video',\n",
       "  'support',\n",
       "  'elephant',\n",
       "  'riding',\n",
       "  'business',\n",
       "  'anymore',\n",
       "  'could',\n",
       "  'take',\n",
       "  'back',\n",
       "  'would',\n",
       "  'instead',\n",
       "  'want',\n",
       "  'pass',\n",
       "  'knowledge',\n",
       "  'anyone',\n",
       "  'aware',\n",
       "  'info',\n",
       "  'show'],\n",
       " ['allison',\n",
       "  'anderson',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'spent',\n",
       "  'days',\n",
       "  'cruising',\n",
       "  'coast',\n",
       "  'alaska',\n",
       "  'magical',\n",
       "  'alaska',\n",
       "  'blog',\n",
       "  'post',\n",
       "  'https',\n",
       "  'allisonanderson',\n",
       "  'com',\n",
       "  'blog',\n",
       "  'crui',\n",
       "  'adventures',\n",
       "  'instagram',\n",
       "  'http',\n",
       "  'www',\n",
       "  'instagram',\n",
       "  'com',\n",
       "  'photoallison',\n",
       "  'show'],\n",
       " ['yes',\n",
       "  'theory',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'black',\n",
       "  'friday',\n",
       "  'drop',\n",
       "  'http',\n",
       "  'seek',\n",
       "  'discomfort',\n",
       "  'com',\n",
       "  'yes',\n",
       "  'theory',\n",
       "  'week',\n",
       "  'every',\n",
       "  'purchase',\n",
       "  'get',\n",
       "  'free',\n",
       "  'seek',\n",
       "  'discomfort',\n",
       "  'flags',\n",
       "  'check',\n",
       "  'friends',\n",
       "  'beautiful',\n",
       "  'destinations',\n",
       "  'videos',\n",
       "  'incredible',\n",
       "  'show'],\n",
       " ['tanya',\n",
       "  'khanijow',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'presenting',\n",
       "  'first',\n",
       "  'part',\n",
       "  'bhutan',\n",
       "  'series',\n",
       "  'episode',\n",
       "  'paro',\n",
       "  'went',\n",
       "  'straight',\n",
       "  'paro',\n",
       "  'first',\n",
       "  'part',\n",
       "  'road',\n",
       "  'trip',\n",
       "  'country',\n",
       "  'drive',\n",
       "  'phuntsholing',\n",
       "  'took',\n",
       "  'hours',\n",
       "  'entire',\n",
       "  'budget',\n",
       "  'bhutan',\n",
       "  'trip',\n",
       "  'close',\n",
       "  'inr',\n",
       "  'k',\n",
       "  'carry',\n",
       "  'cash',\n",
       "  'everywhere',\n",
       "  'indian',\n",
       "  'currency',\n",
       "  'bhutan',\n",
       "  'accepted',\n",
       "  'things',\n",
       "  'paro',\n",
       "  'place',\n",
       "  'stayed',\n",
       "  'paro',\n",
       "  'called',\n",
       "  'ama',\n",
       "  'village',\n",
       "  'lodge',\n",
       "  'book',\n",
       "  'place',\n",
       "  'show']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a023bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tanya',\n",
       "  'khanijow',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'journey',\n",
       "  'arunachal',\n",
       "  'north',\n",
       "  'east',\n",
       "  'india',\n",
       "  'begins',\n",
       "  'train',\n",
       "  'journey',\n",
       "  'guwahati',\n",
       "  'murkongselek',\n",
       "  'head',\n",
       "  'pasighat',\n",
       "  'travel',\n",
       "  'companions',\n",
       "  'getting',\n",
       "  'started',\n",
       "  'exploring',\n",
       "  'tiny',\n",
       "  'glimpse',\n",
       "  'arunachal',\n",
       "  'far',\n",
       "  'markets',\n",
       "  'bridges',\n",
       "  'adventure',\n",
       "  'get',\n",
       "  'better',\n",
       "  'next',\n",
       "  'video',\n",
       "  'show'],\n",
       " ['priscilla',\n",
       "  'lee',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'disclaimer',\n",
       "  'please',\n",
       "  'ride',\n",
       "  'elephants',\n",
       "  'visiting',\n",
       "  'country',\n",
       "  'time',\n",
       "  'know',\n",
       "  'yes',\n",
       "  'dumb',\n",
       "  'shown',\n",
       "  'video',\n",
       "  'support',\n",
       "  'elephant',\n",
       "  'riding',\n",
       "  'business',\n",
       "  'anymore',\n",
       "  'could',\n",
       "  'take',\n",
       "  'back',\n",
       "  'would',\n",
       "  'instead',\n",
       "  'want',\n",
       "  'pass',\n",
       "  'knowledge',\n",
       "  'anyone',\n",
       "  'aware',\n",
       "  'info',\n",
       "  'show'],\n",
       " ['allison',\n",
       "  'anderson',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'spent',\n",
       "  'days',\n",
       "  'cruising',\n",
       "  'coast',\n",
       "  'alaska',\n",
       "  'magical',\n",
       "  'alaska',\n",
       "  'blog',\n",
       "  'post',\n",
       "  'https',\n",
       "  'allisonanderson',\n",
       "  'com',\n",
       "  'blog',\n",
       "  'crui',\n",
       "  'adventures',\n",
       "  'instagram',\n",
       "  'http',\n",
       "  'www',\n",
       "  'instagram',\n",
       "  'com',\n",
       "  'photoallison',\n",
       "  'show'],\n",
       " ['yes',\n",
       "  'theory',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'black',\n",
       "  'friday',\n",
       "  'drop',\n",
       "  'http',\n",
       "  'seek',\n",
       "  'discomfort',\n",
       "  'com',\n",
       "  'yes',\n",
       "  'theory',\n",
       "  'week',\n",
       "  'every',\n",
       "  'purchase',\n",
       "  'get',\n",
       "  'free',\n",
       "  'seek',\n",
       "  'discomfort',\n",
       "  'flags',\n",
       "  'check',\n",
       "  'friends',\n",
       "  'beautiful',\n",
       "  'destinations',\n",
       "  'videos',\n",
       "  'incredible',\n",
       "  'show'],\n",
       " ['tanya',\n",
       "  'khanijow',\n",
       "  'k',\n",
       "  'subscribers',\n",
       "  'subscribe',\n",
       "  'presenting',\n",
       "  'first',\n",
       "  'part',\n",
       "  'bhutan',\n",
       "  'series',\n",
       "  'episode',\n",
       "  'paro',\n",
       "  'went',\n",
       "  'straight',\n",
       "  'paro',\n",
       "  'first',\n",
       "  'part',\n",
       "  'road',\n",
       "  'trip',\n",
       "  'country',\n",
       "  'drive',\n",
       "  'phuntsholing',\n",
       "  'took',\n",
       "  'hours',\n",
       "  'entire',\n",
       "  'budget',\n",
       "  'bhutan',\n",
       "  'trip',\n",
       "  'close',\n",
       "  'inr',\n",
       "  'k',\n",
       "  'carry',\n",
       "  'cash',\n",
       "  'everywhere',\n",
       "  'indian',\n",
       "  'currency',\n",
       "  'bhutan',\n",
       "  'accepted',\n",
       "  'things',\n",
       "  'paro',\n",
       "  'place',\n",
       "  'stayed',\n",
       "  'paro',\n",
       "  'called',\n",
       "  'ama',\n",
       "  'village',\n",
       "  'lodge',\n",
       "  'book',\n",
       "  'place',\n",
       "  'show']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "574edb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_new.shape[0]==len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52a90d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af279279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('huge', 0.9954676628112793), ('rest', 0.9946334958076477), ('big', 0.9945908784866333), ('order', 0.9945046901702881), ('peace', 0.9945025444030762), ('quick', 0.9943442344665527), ('headed', 0.9942252039909363), ('eleven', 0.9940177202224731), ('dream', 0.9939081072807312), ('chen', 0.9938148856163025)]\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∞—Å—å\n",
    "print(model_imdb.wv.most_similar(positive=['adventure'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7cea9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    –î–ª—è —Ç–µ–∫—Å—Ç–∞ —É—Å—Ä–µ–¥–Ω–∏–º –≤–µ–∫—Ç–æ—Ä–∞ –≤—Ö–æ–¥—è—â–∏—Ö –≤ –Ω–µ–≥–æ —Å–ª–æ–≤\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5752ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç–∫–∞ \t Accuracy\n",
      "art_music \t 0.4925373134328358\n",
      "food \t 0.2803030303030303\n",
      "history \t 0.13559322033898305\n",
      "travel \t 0.5876010781671159\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a29fa113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç–∫–∞ \t Accuracy\n",
      "art_music \t 0.48880597014925375\n",
      "food \t 0.3712121212121212\n",
      "history \t 0.1638418079096045\n",
      "travel \t 0.522911051212938\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1d4ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç–∫–∞ \t Accuracy\n",
      "art_music \t 0.5522388059701493\n",
      "food \t 0.4090909090909091\n",
      "history \t 0.3446327683615819\n",
      "travel \t 0.40431266846361186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "sentiment(EmbeddingVectorizer(model_imdb.wv), KNeighborsClassifier(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6681fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    y_true - –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "    y_pred - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: –∫–ª—é—á - –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞, \n",
    "    –∑–Ω–∞—á–µ–Ω–∏–µ - Accuracy –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    \"\"\"\n",
    "    # –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "    classes = np.unique(y_true)\n",
    "    # –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Å–ª–æ–≤–∞—Ä—å\n",
    "    res = dict()\n",
    "    # –ü–µ—Ä–µ–±–æ—Ä –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "    for c in classes:\n",
    "        # –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç \n",
    "        # —Ç–µ–∫—É—â–µ–π –º–µ—Ç–∫–µ –∫–ª–∞—Å—Å–∞ –≤ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # —Ä–∞—Å—á–µ—Ç accuracy –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫–∏ accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('–ú–µ—Ç–∫–∞ \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2552cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞—é—â–∞—è –∏ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∏\n",
    "boundary = 3000\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = imdb_df.category.values[:boundary]\n",
    "y_test = imdb_df.category.values[boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f6fef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–µ—Ç–∫–∞ \t Accuracy\n",
      "art_music \t 0.6666666666666666\n",
      "history \t 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7798d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>Ranveer Allahbadia\\n2.53M subscribers\\nSUBSCRI...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>Knowledgia\\n650K subscribers\\nSUBSCRIBE\\nTHE H...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>Let's Crack UPSC CSE\\n4.71M subscribers\\nSUBSC...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>Abhijit Chavda\\n74.8K subscribers\\nSUBSCRIBE\\n...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>PDF visuals\\n98.8K subscribers\\nSUBSCRIBE\\nHer...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>CrashCourse\\n12.4M subscribers\\nSUBSCRIBE\\nThe...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>Publications Office of the European Union\\n3.2...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>History Time\\n619K subscribers\\nSUBSCRIBE\\n- W...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>Mr. Raymond's Civics and Social Studies Academ...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>Paul Sargent\\n25.3K subscribers\\nSUBSCRIBE\\nIn...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>593 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    value\n",
       "3006  Ranveer Allahbadia\\n2.53M subscribers\\nSUBSCRI...  history\n",
       "3007  Knowledgia\\n650K subscribers\\nSUBSCRIBE\\nTHE H...  history\n",
       "3008  Let's Crack UPSC CSE\\n4.71M subscribers\\nSUBSC...  history\n",
       "3009  Abhijit Chavda\\n74.8K subscribers\\nSUBSCRIBE\\n...  history\n",
       "3010  PDF visuals\\n98.8K subscribers\\nSUBSCRIBE\\nHer...  history\n",
       "...                                                 ...      ...\n",
       "3594  CrashCourse\\n12.4M subscribers\\nSUBSCRIBE\\nThe...  history\n",
       "3595  Publications Office of the European Union\\n3.2...  history\n",
       "3596  History Time\\n619K subscribers\\nSUBSCRIBE\\n- W...  history\n",
       "3597  Mr. Raymond's Civics and Social Studies Academ...  history\n",
       "3598  Paul Sargent\\n25.3K subscribers\\nSUBSCRIBE\\nIn...  history\n",
       "\n",
       "[593 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new['value']=='history']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
